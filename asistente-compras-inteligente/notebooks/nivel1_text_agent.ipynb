{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1c228bb",
   "metadata": {},
   "source": [
    "### Instalaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d474de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalación de Librerías (ejecutar solo una vez si no están instaladas)\n",
    "# !pip install langgraph langchain langchain_aws boto3 python-dotenv anthropic # Añadimos anthropic por si usamos Claude directamente\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25ddbed",
   "metadata": {},
   "source": [
    "### Importar librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd49f5af",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import json\n",
    "from typing import TypedDict, Annotated, List, Union\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, AIMessage \n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver \n",
    "from langchain_aws import ChatBedrock\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "# from dotenv import load_dotenv\n",
    "# load_dotenv() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33f955e",
   "metadata": {},
   "source": [
    "### Configuración del Cliente de Bedrock y LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99a47e3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    \n",
    "    bedrock_runtime_client = boto3.client(\n",
    "        service_name=\"bedrock-runtime\",\n",
    "        # region_name=region_name \n",
    "    )\n",
    "    print(\"Cliente de Bedrock Runtime inicializado exitosamente.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error inicializando el cliente de Bedrock Runtime: {e}\")\n",
    "    print(\"Por favor, verifica tu configuración de credenciales y región de AWS.\")\n",
    "    \n",
    "\n",
    "\n",
    "MODEL_ID_HAIKU = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "MODEL_ID_SONNET = \"anthropic.claude-3-sonnet-20240229-v1:0\" \n",
    "\n",
    "\n",
    "try:\n",
    "    llm = ChatBedrock(\n",
    "        client=bedrock_runtime_client, \n",
    "        model_id=MODEL_ID_HAIKU,\n",
    "        model_kwargs={\n",
    "            \"temperature\": 0.1, \n",
    "            \"max_tokens\": 1024    \n",
    "        }\n",
    "    )\n",
    "    print(f\"LLM ({MODEL_ID_HAIKU}) inicializado exitosamente con ChatBedrock.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error inicializando ChatBedrock con {MODEL_ID_HAIKU}: {e}\")\n",
    "    print(\"Asegúrate de que el modelo está habilitado en tu cuenta de Bedrock para la región configurada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac0671c",
   "metadata": {},
   "source": [
    "### Definición del Estado del Grafo (AgentState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c6a23d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class AgentState(TypedDict):\n",
    "    userInput: str\n",
    "    intent: str\n",
    "    entities: dict\n",
    "    catalogQueryResult: List[dict]\n",
    "    finalResponse: str\n",
    "    callLog: List[str] \n",
    "    # (Opcional para Nivel 1 ) supervisor_notes: str\n",
    "    # (Opcional para Nivel 1 ) error_message: str \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef15ff5",
   "metadata": {},
   "source": [
    "### Definición del Nodo - Interpretar Entrada del Usuario (NLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ac606a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def interpret_user_input(state: AgentState):\n",
    "    \"\"\"\n",
    "    Toma la entrada del usuario y usa el LLM para extraer la intención y las entidades.\n",
    "    \"\"\"\n",
    "    print(\"---NODO: Interpretando Entrada del Usuario---\")\n",
    "    user_input = state[\"userInput\"]\n",
    "    current_call_log = state.get(\"callLog\", [])\n",
    "\n",
    "\n",
    "    prompt_template = ChatPromptTemplate.from_messages([\n",
    "        SystemMessage(\n",
    "            content=(\n",
    "                \"Eres un asistente experto en procesamiento de lenguaje natural para un sistema de compras. \"\n",
    "                \"Tu tarea es analizar la consulta del usuario y extraer su intención principal y las entidades relevantes. \"\n",
    "                \"Las intenciones pueden ser: 'buscar_producto', 'comparar_productos', 'pedir_recomendacion', 'ver_carrito', 'saludar', 'despedirse', 'otra'. \"\n",
    "                \"Las entidades comunes son: 'nombre_producto', 'marca', 'categoria', 'color', 'talla', 'precio_maximo', 'caracteristicas_adicionales'. \"\n",
    "                \"Responde ÚNICAMENTE con un objeto JSON que contenga dos claves: 'intent' y 'entities'. \"\n",
    "                \"La clave 'entities' debe ser un diccionario de las entidades encontradas. Si no encuentras una entidad, no la incluyas. \"\n",
    "                \"Si no estás seguro de la intención o no puedes extraer entidades útiles, puedes usar la intención 'otra' y un diccionario de entidades vacío.\"\n",
    "                \"Ejemplo de salida: {\\\"intent\\\": \\\"buscar_producto\\\", \\\"entities\\\": {\\\"categoria\\\": \\\"televisor\\\", \\\"marca\\\": \\\"LG\\\", \\\"tamaño\\\": \\\"55 pulgadas\\\"}}\"\n",
    "            )\n",
    "        ),\n",
    "        HumanMessage(content=f\"Analiza la siguiente consulta del usuario: {user_input}\")\n",
    "    ])\n",
    "\n",
    "    try:\n",
    "        formatted_prompt = prompt_template.format_messages(userInput=user_input)\n",
    "        ai_response = llm.invoke(formatted_prompt) \n",
    "\n",
    "        response_content = ai_response.content\n",
    "        print(f\"Respuesta cruda del LLM para NLU: {response_content}\")\n",
    "\n",
    "        parsed_response = json.loads(response_content)\n",
    "        intent = parsed_response.get(\"intent\", \"otra\")\n",
    "        entities = parsed_response.get(\"entities\", {})\n",
    "\n",
    "        current_call_log.append(f\"NLU: Input='{user_input}', Intent='{intent}', Entities='{json.dumps(entities)}', LLM_Raw_Output='{response_content[:200]}...'\") \n",
    "        print(f\"Intención extraída: {intent}\")\n",
    "        print(f\"Entidades extraídas: {entities}\")\n",
    "\n",
    "        return {\n",
    "            \"intent\": intent,\n",
    "            \"entities\": entities,\n",
    "            \"callLog\": current_call_log\n",
    "        }\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error al decodificar la respuesta JSON del LLM: {e}\")\n",
    "        print(f\"Respuesta del LLM que causó el error: {response_content}\")\n",
    "        current_call_log.append(f\"NLU_ERROR: Error decodificando JSON. LLM_Raw_Output='{response_content[:200]}...'\")\n",
    "        return {\n",
    "            \"intent\": \"error_nlu\",\n",
    "            \"entities\": {},\n",
    "            \"callLog\": current_call_log\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error inesperado durante la NLU: {e}\")\n",
    "        current_call_log.append(f\"NLU_ERROR: Error inesperado - {str(e)}\")\n",
    "        return {\n",
    "            \"intent\": \"error_nlu\",\n",
    "            \"entities\": {},\n",
    "            \"callLog\": current_call_log\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218395bd",
   "metadata": {},
   "source": [
    "### Definición del Nodo - Consultar Catálogo de Productos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40473a58",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import json \n",
    "import os   \n",
    "\n",
    "def load_product_database(db_path=\"../data/products.json\"):\n",
    "    \"\"\"\n",
    "    Carga la base de datos de productos desde un archivo JSON.\n",
    "    Asume que el archivo products.json está en una carpeta 'data'\n",
    "    ubicada un nivel arriba de la carpeta donde se encuentra este notebook.\n",
    "    \"\"\"\n",
    "   \n",
    "    # Para verificar la ruta actual desde donde se ejecuta el notebook \n",
    "    # print(f\"Directorio de trabajo actual: {os.getcwd()}\")\n",
    "    # print(f\"Intentando cargar DB desde: {os.path.abspath(db_path)}\")\n",
    "\n",
    "    try:\n",
    "        with open(db_path, 'r', encoding='utf-8') as f:\n",
    "            database = json.load(f)\n",
    "        print(f\"Base de datos de productos cargada exitosamente desde '{os.path.abspath(db_path)}'\")\n",
    "        return database\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR CRÍTICO: Archivo de base de datos no encontrado en '{os.path.abspath(db_path)}'.\")\n",
    "        print(\"Asegúrate de que la carpeta 'data' exista en la raíz de tu proyecto y contenga 'products.json'.\")\n",
    "        print(f\"Directorio actual: {os.getcwd()}\")\n",
    "        return []\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"ERROR CRÍTICO: Error al decodificar el archivo JSON en '{os.path.abspath(db_path)}'. Verifica el formato del archivo.\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR CRÍTICO: Ocurrió un error inesperado al cargar la base de datos: {e}\")\n",
    "        return []\n",
    "\n",
    "def query_product_catalog(state: AgentState):\n",
    "    \"\"\"\n",
    "    Consulta un catálogo de productos (cargado desde un archivo JSON) basado en las entidades extraídas.\n",
    "    \"\"\"\n",
    "    print(\"---NODO: Consultando Catálogo de Productos (desde Archivo JSON)---\")\n",
    "    entities = state.get(\"entities\", {})\n",
    "    intent = state.get(\"intent\", \"\")\n",
    "    current_call_log = state.get(\"callLog\", [])\n",
    "\n",
    "    \n",
    "    product_database = load_product_database() \n",
    "\n",
    "    if not product_database: \n",
    "        current_call_log.append(\"CATALOG_ERROR: La base de datos de productos no se pudo cargar o está vacía.\")\n",
    "        print(\"Error: La base de datos de productos está vacía o no se pudo cargar. Revisa los mensajes anteriores.\")\n",
    "        return {\"catalogQueryResult\": [], \"callLog\": current_call_log}\n",
    "\n",
    "    \n",
    "    if intent not in [\"buscar_producto\", \"pedir_recomendacion\", \"comparar_productos\"]:\n",
    "        print(f\"Intención '{intent}' no requiere consulta de catálogo. Omitiendo.\")\n",
    "        current_call_log.append(f\"CATALOG_SKIP: Intención '{intent}' no requiere consulta.\")\n",
    "        return {\"catalogQueryResult\": [], \"callLog\": current_call_log}\n",
    "\n",
    "    print(f\"Consultando catálogo con entidades: {entities}\")\n",
    "    \n",
    "    results = []\n",
    "    if not entities:\n",
    "        print(\"No se proporcionaron entidades para filtrar. Considera devolver algunos productos populares o ninguno.\")\n",
    "        # results = product_database[:3] \n",
    "        results = [] \n",
    "    else:\n",
    "        for product in product_database:\n",
    "            match = True\n",
    "            # Comprobar categoría\n",
    "            entity_categoria = entities.get(\"categoria\", \"\").lower()\n",
    "            product_categoria = product.get(\"categoria\", \"\").lower()\n",
    "            if entity_categoria and entity_categoria not in product_categoria:\n",
    "                match = False\n",
    "            \n",
    "            # Comprobar marca\n",
    "            entity_marca = entities.get(\"marca\", \"\").lower()\n",
    "            product_marca = product.get(\"marca\", \"\").lower()\n",
    "            if entity_marca and entity_marca not in product_marca:\n",
    "                match = False\n",
    "            \n",
    "            # Comprobar nombre_producto \n",
    "            entity_nombre = entities.get(\"nombre_producto\", \"\").lower()\n",
    "            product_nombre = product.get(\"nombre\", \"\").lower()\n",
    "            if entity_nombre and entity_nombre not in product_nombre: \n",
    "                match = False\n",
    "            \n",
    "            # Comprobar color \n",
    "            entity_color = entities.get(\"color\", \"\").lower()\n",
    "            product_colores = [str(c).lower() for c in product.get(\"colores\", [])] # Asegurar que sean strings\n",
    "            if entity_color and product_colores and entity_color not in product_colores:\n",
    "                match = False\n",
    "            \n",
    "            # Comprobar talla \n",
    "            entity_talla = str(entities.get(\"talla\", \"\")) \n",
    "            product_tallas = [str(t) for t in product.get(\"tallas_disponibles\", [])] \n",
    "            if entity_talla and product_tallas and entity_talla not in product_tallas:\n",
    "                match = False\n",
    "            \n",
    "            if match:\n",
    "                results.append(product)\n",
    "\n",
    "    if not results:\n",
    "        print(\"No se encontraron productos que coincidan con las entidades.\")\n",
    "        current_call_log.append(f\"CATALOG_QUERY: Entities='{json.dumps(entities)}', Result='No products found'\")\n",
    "    else:\n",
    "        print(f\"Productos encontrados: {len(results)}\")\n",
    "        summary_results = [{\"id\": p.get(\"id\"), \"nombre\": p.get(\"nombre\")} for p in results[:3]]\n",
    "        current_call_log.append(f\"CATALOG_QUERY: Entities='{json.dumps(entities)}', Found='{len(results)} items', ExampleResults='{json.dumps(summary_results)}'\")\n",
    "\n",
    "    return {\n",
    "        \"catalogQueryResult\": results,\n",
    "        \"callLog\": current_call_log\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3859a9c8",
   "metadata": {},
   "source": [
    "### Definición del Nodo - Generar Respuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a4987c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def generate_response(state: AgentState):\n",
    "    \"\"\"\n",
    "    Genera una respuesta en lenguaje natural basada en los resultados de la consulta al catálogo\n",
    "    y la intención/entidades del usuario.\n",
    "    \"\"\"\n",
    "    print(\"---NODO: Generando Respuesta---\")\n",
    "    user_input = state.get(\"userInput\", \"\")\n",
    "    intent = state.get(\"intent\", \"\")\n",
    "    entities = state.get(\"entities\", {})\n",
    "    catalog_results = state.get(\"catalogQueryResult\", [])\n",
    "    current_call_log = state.get(\"callLog\", [])\n",
    "\n",
    "    # Preparar el contexto para el LLM\n",
    "    context_for_llm = f\"La consulta original del usuario fue: '{user_input}'.\\n\"\n",
    "    context_for_llm += f\"La intención identificada fue: '{intent}'.\\n\"\n",
    "    if entities:\n",
    "        context_for_llm += f\"Las entidades extraídas fueron: {json.dumps(entities)}.\\n\"\n",
    "\n",
    "    if intent == \"error_nlu\":\n",
    "        final_response_text = \"Lo siento, tuve problemas para entender tu solicitud. ¿Podrías intentarlo de nuevo de otra manera?\"\n",
    "        current_call_log.append(f\"RESPONSE_GEN: Intent='{intent}', Generated_Response='{final_response_text}'\")\n",
    "        return {\"finalResponse\": final_response_text, \"callLog\": current_call_log}\n",
    "\n",
    "    if intent in [\"saludar\", \"despedirse\"] and not catalog_results: \n",
    "        if intent == \"saludar\":\n",
    "            final_response_text = \"¡Hola! Soy tu asistente de compras. ¿En qué puedo ayudarte hoy?\"\n",
    "        elif intent == \"despedirse\":\n",
    "            final_response_text = \"¡Hasta luego! Que tengas un buen día.\"\n",
    "        else: # Por si acaso\n",
    "            final_response_text = \"Entendido.\"\n",
    "        current_call_log.append(f\"RESPONSE_GEN: Intent='{intent}', Generated_Response='{final_response_text}'\")\n",
    "        return {\"finalResponse\": final_response_text, \"callLog\": current_call_log}\n",
    "\n",
    "\n",
    "    if not catalog_results:\n",
    "        if intent in [\"buscar_producto\", \"pedir_recomendacion\", \"comparar_productos\"]:\n",
    "            context_for_llm += \"No se encontraron productos que coincidan exactamente con tu búsqueda en el catálogo.\\n\"\n",
    "            system_message_content = (\n",
    "                \"Eres un asistente de compras amigable y servicial. \"\n",
    "                \"El usuario realizó una búsqueda pero no se encontraron productos. \"\n",
    "                \"Informa al usuario de esto de manera amigable y quizás sugiere que intente una búsqueda diferente o más general. \"\n",
    "                \"No inventes productos. Sé breve y directo.\"\n",
    "            )\n",
    "        else:\n",
    "            context_for_llm += \"No se requirió ni se obtuvo información del catálogo para esta consulta.\\n\"\n",
    "            system_message_content = (\n",
    "                \"Eres un asistente de compras amigable y servicial. \"\n",
    "                \"Responde al usuario de forma concisa basándote en la intención y el contexto proporcionado. \"\n",
    "                \"Si la intención no está clara, pide una aclaración.\"\n",
    "            )\n",
    "    else:\n",
    "        context_for_llm += \"Se encontraron los siguientes productos en el catálogo que podrían ser relevantes:\\n\"\n",
    "        for i, product in enumerate(catalog_results[:3]): \n",
    "            context_for_llm += f\"  Producto {i+1}: {product.get('nombre', 'Nombre no disponible')} (Marca: {product.get('marca', 'N/A')}, Precio: ${product.get('precio', 'N/A')})\\n\"\n",
    "            if product.get('descripcion'):\n",
    "                 context_for_llm += f\"    Descripción: {product.get('descripcion')}\\n\"\n",
    "        if len(catalog_results) > 3:\n",
    "            context_for_llm += f\"  ... y {len(catalog_results) - 3} productos más.\\n\"\n",
    "        \n",
    "        system_message_content = (\n",
    "            \"Eres un asistente de compras amigable y servicial. \"\n",
    "            \"Basándote en la consulta del usuario y los productos encontrados en el catálogo (proporcionados en el contexto), \"\n",
    "            \"genera una respuesta útil y conversacional. Resume la información si es necesario. \"\n",
    "            \"Si hay varios productos, puedes mencionar algunos y preguntar si el usuario desea más detalles sobre alguno en particular. \"\n",
    "            \"No inventes productos ni características que no estén en la lista.\"\n",
    "        )\n",
    "\n",
    "    prompt_template = ChatPromptTemplate.from_messages([\n",
    "        SystemMessage(content=system_message_content),\n",
    "        HumanMessage(content=context_for_llm + \"\\nPor favor, genera una respuesta adecuada para el usuario.\")\n",
    "    ])\n",
    "\n",
    "    try:\n",
    "        formatted_prompt = prompt_template.format_messages() \n",
    "        ai_response = llm.invoke(formatted_prompt)\n",
    "        final_response_text = ai_response.content.strip()\n",
    "\n",
    "        print(f\"Respuesta generada por el LLM: {final_response_text}\")\n",
    "        current_call_log.append(f\"RESPONSE_GEN: Intent='{intent}', Found_Items='{len(catalog_results)}', Generated_Response='{final_response_text[:200]}...'\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error durante la generación de respuesta con el LLM: {e}\")\n",
    "        current_call_log.append(f\"RESPONSE_GEN_ERROR: Error - {str(e)}\")\n",
    "        final_response_text = \"Lo siento, estoy teniendo problemas para generar una respuesta en este momento. Intenta de nuevo más tarde.\"\n",
    "\n",
    "    return {\n",
    "        \"finalResponse\": final_response_text,\n",
    "        \"callLog\": current_call_log\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cfbd99",
   "metadata": {},
   "source": [
    "### Ensamblaje del Grafo LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b4167d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "workflow.add_node(\"nlu_parser\", interpret_user_input)\n",
    "workflow.add_node(\"catalog_tool\", query_product_catalog)\n",
    "workflow.add_node(\"response_generator\", generate_response)\n",
    "\n",
    "workflow.set_entry_point(\"nlu_parser\")\n",
    "\n",
    "workflow.add_edge(\"nlu_parser\", \"catalog_tool\")\n",
    "\n",
    "workflow.add_edge(\"catalog_tool\", \"response_generator\")\n",
    "\n",
    "workflow.add_edge(\"response_generator\", END)\n",
    "\n",
    "\n",
    "try:\n",
    "    app = workflow.compile()\n",
    "    print(\"Grafo LangGraph compilado exitosamente.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al compilar el grafo LangGraph: {e}\")\n",
    "    app = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded23661",
   "metadata": {},
   "source": [
    "### Ejecutar el Agente con una Entrada de Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34ba93e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Solo ejecutar si el grafo se compiló correctamente\n",
    "if app:\n",
    "    print(\"\\n--- EJECUTANDO EL AGENTE ---\")\n",
    "    \n",
    "    # Definir la entrada del usuario\n",
    "    user_query = \"Hola, estoy buscando un televisor SuperVision de 55 pulgadas, ¿tienes alguno?\"\n",
    "    # user_query = \"Tienes zapatillas RunnerFlex rojas talla 42?\"\n",
    "    # user_query = \"Qué tal?\"\n",
    "    # user_query = \"Adiós\"\n",
    "    # user_query = \"Busco un taladro percutor\" # Ejemplo de algo que no está en el catálogo\n",
    "\n",
    "    initial_state_input = {\"userInput\": user_query, \"callLog\": []} \n",
    "\n",
    "    print(f\"Entrada del Usuario: \\\"{user_query}\\\"\")\n",
    "    \n",
    "    # Invocar el agente con la entrada.\n",
    "    \n",
    "    try:\n",
    "        final_state = app.invoke(initial_state_input)\n",
    "\n",
    "        # Mostrar la respuesta final y el log de llamadas\n",
    "        print(\"\\n--- RESULTADO FINAL DEL AGENTE ---\")\n",
    "        print(f\"Respuesta Final para el Usuario: {final_state.get('finalResponse', 'No se generó respuesta final.')}\")\n",
    "        \n",
    "        print(\"\\n--- LOG DE LLAMADAS (Rendimiento/Supervisión) ---\")\n",
    "        if final_state.get('callLog'):\n",
    "            for log_entry in final_state['callLog']:\n",
    "                print(log_entry)\n",
    "        else:\n",
    "            print(\"El log de llamadas está vacío.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error durante la ejecución del agente: {e}\")\n",
    "        print(\"Detalles del estado en el momento del error (si está disponible en la excepción):\")\n",
    "        \n",
    "        # Algunas excepciones de LangGraph pueden llevar el estado parcial.\n",
    "        if hasattr(e, 'state'):\n",
    "             print(json.dumps(e.state, indent=2, ensure_ascii=False))\n",
    "\n",
    "else:\n",
    "    print(\"El agente no se pudo compilar. No se puede ejecutar.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ac650b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
